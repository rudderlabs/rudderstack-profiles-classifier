import os
import json
import pathlib
import pandas as pd
from typing import Any, List, Tuple, Union

import warnings
from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning

warnings.filterwarnings('ignore', category=NumbaDeprecationWarning)
warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)

import utils
from logger import logger
import constants

metrics_table = constants.METRICS_TABLE
model_file_name = constants.MODEL_FILE_NAME

def train_and_store_model_results_rs(feature_table_name: str,
            merged_config: dict, **kwargs) -> dict:
    """Creates and saves the trained model pipeline after performing preprocessing and classification and returns the model id attached with the results generated.

    Args:
        feature_table_name (str): name of the user feature table generated by profiles feature table model, and is input to training and prediction
        merged_config (dict): configs from profiles.yaml which should overwrite corresponding values from model_configs.yaml file
        From kwargs:
            session (redshift_connector.cursor.Cursor): valid redshift session to access data warehouse
            connector (RedshiftConnector): RedshiftConnector object
            trainer (MLTrainer): MLTrainer object which has all the configs and methods required for training

    Returns:
        dict: returns the model_id which is basically the time converted to key at which results were generated along with precision, recall, fpr and tpr to generate pr-auc and roc-auc curve.
    """
    session = kwargs.get("session")
    connector = kwargs.get("connector")
    trainer = kwargs.get("trainer")
    if session == None or connector == None or trainer == None:
        raise ValueError("session, connector and trainer are required in kwargs for training in Redshift")
    model_file = connector.join_file_path(f"{trainer.output_profiles_ml_model}_{model_file_name}")
    feature_df_path = connector.fetch_feature_df_path(feature_table_name)
    feature_df = pd.read_parquet(feature_df_path)
    feature_df.columns = [col.upper() for col in feature_df.columns]

    stringtype_features = connector.get_stringtype_features(feature_df, trainer.label_column, trainer.entity_column, session=session)
    categorical_columns = utils.merge_lists_to_unique(trainer.prep.categorical_pipeline['categorical_columns'], stringtype_features)

    non_stringtype_features = connector.get_non_stringtype_features(feature_df, trainer.label_column, trainer.entity_column, session=session)
    numeric_columns = utils.merge_lists_to_unique(trainer.prep.numeric_pipeline['numeric_columns'], non_stringtype_features)
    
    train_x, test_x, test_y, pipe, model_id, metrics_df, results = trainer.train_model( feature_df, categorical_columns, numeric_columns, merged_config, model_file)

    column_dict = {'numeric_columns': numeric_columns, 'categorical_columns': categorical_columns}
    column_name_file = connector.join_file_path(f"{trainer.output_profiles_ml_model}_{model_id}_column_names.json")
    json.dump(column_dict, open(column_name_file,"w"))

    trainer.plot_diagnostics(connector, session, pipe, None, test_x, test_y, trainer.label_column)
    try:
        figure_file = connector.join_file_path(trainer.figure_names['feature-importance-chart'])
        shap_importance = utils.plot_top_k_feature_importance(pipe, train_x, numeric_columns, categorical_columns, figure_file, top_k_features=5)
        connector.write_pandas(shap_importance, f"FEATURE_IMPORTANCE", if_exists="replace")
    except Exception as e:
        logger.error(f"Could not generate plots {e}")

    database_dtypes = json.loads(constants.rs_dtypes)
    metrics_table_query = ""
    for col in metrics_df.columns:
        if metrics_df[col].dtype == 'object':
            metrics_df[col] = metrics_df[col].apply(lambda x: json.dumps(x))
            metrics_table_query += f"{col} {database_dtypes['text']},"
        elif metrics_df[col].dtype == 'float64' or metrics_df[col].dtype == 'int64':
            metrics_table_query += f"{col} {database_dtypes['num']},"
        elif metrics_df[col].dtype == 'bool':
            metrics_table_query += f"{col} {database_dtypes['bool']},"
        elif metrics_df[col].dtype == 'datetime64[ns]':
            metrics_table_query += f"{col} {database_dtypes['timestamp']},"
    metrics_table_query = metrics_table_query[:-1]
        
    connector.run_query(session, f"CREATE TABLE IF NOT EXISTS {metrics_table} ({metrics_table_query});", response=False)
    connector.write_pandas(metrics_df, f"{metrics_table}", if_exists="append")
    return results

def _prepare_feature_table(feature_table_name: str, 
                        label_table_name: str,
                        cardinal_feature_threshold: float, **kwargs) -> tuple:
    """This function creates a feature table as per the requirement of customer that is further used for training and prediction.

    Args:
        feature_table_name (str): feature table from the retrieved material_names tuple
        label_table_name (str): label table from the retrieved material_names tuple
    Returns:
        snowflake.snowpark.Table: feature table made using given instance from material names
    """
    session = kwargs.get('session', None)
    connector = kwargs.get('connector', None)
    trainer = kwargs.get('trainer', None)
    try:
        label_ts_col = f"{trainer.index_timestamp}_label_ts"
        if trainer.eligible_users:
            feature_table = connector.get_table(session, feature_table_name, filter_condition=trainer.eligible_users)
        else:
            default_user_shortlisting = f"{trainer.label_column} != {trainer.label_value}"
            feature_table = connector.get_table(session, feature_table_name, filter_condition=default_user_shortlisting) #.withColumn(label_ts_col, F.dateadd("day", F.lit(prediction_horizon_days), F.col(index_timestamp)))
        arraytype_features = connector.get_arraytype_features(session, feature_table_name)
        ignore_features = utils.merge_lists_to_unique(trainer.prep.ignore_features, arraytype_features)
        high_cardinal_features = connector.get_high_cardinal_features(feature_table, trainer.label_column, trainer.entity_column, cardinal_feature_threshold)
        ignore_features = utils.merge_lists_to_unique(ignore_features, high_cardinal_features)
        feature_table = connector.drop_cols(feature_table, [trainer.label_column])
        timestamp_columns = trainer.prep.timestamp_columns
        if len(timestamp_columns) == 0:
            timestamp_columns = connector.get_timestamp_columns(session, feature_table_name, trainer.index_timestamp)
        for col in timestamp_columns:
            feature_table = connector.add_days_diff(feature_table, col, col, trainer.index_timestamp)
        label_table = trainer.prepare_label_table(connector, session, label_table_name, label_ts_col)
        uppercase_list = lambda names: [name.upper() for name in names]
        lowercase_list = lambda names: [name.lower() for name in names]
        ignore_features_ = [col for col in feature_table.columns if col in uppercase_list(ignore_features) or col in lowercase_list(ignore_features)]
        trainer.prep.ignore_features = ignore_features_
        trainer.prep.timestamp_columns = timestamp_columns
        feature_table = connector.join_feature_table_label_table(feature_table, label_table, trainer.entity_column, "inner")
        feature_table = connector.drop_cols(feature_table, [label_ts_col])
        feature_table = connector.drop_cols(feature_table, ignore_features_)
        return feature_table, arraytype_features, timestamp_columns
    except Exception as e:
        print("Exception occured while preparing feature table. Please check the logs for more details")
        raise e
    
def preprocess_and_train(train_procedure, material_names: List[Tuple[str]], merged_config: dict, **kwargs):
    session = kwargs.get('session', None)
    connector = kwargs.get('connector', None)
    trainer = kwargs.get('trainer', None)
    min_sample_for_training = constants.MIN_SAMPLES_FOR_TRAINING
    cardinal_feature_threshold = constants.CARDINAL_FEATURE_THRESOLD

    feature_table = None
    for row in material_names:
        feature_table_name, label_table_name = row
        logger.info(f"Preparing training dataset using {feature_table_name} and {label_table_name} as feature and label tables respectively")
        feature_table_instance, arraytype_features, timestamp_columns = _prepare_feature_table(feature_table_name, 
                                                                label_table_name,
                                                                cardinal_feature_threshold,
                                                                session=session,
                                                                connector=connector,
                                                                trainer=trainer)
        if feature_table is None:
            feature_table = feature_table_instance
            break
        else:
            feature_table = feature_table.unionAllByName(feature_table_instance)
    feature_table_name_remote = f"{trainer.output_profiles_ml_model}_features"
    filtered_feature_table = connector.filter_feature_table(feature_table, trainer.entity_column, 
                                                                    trainer.index_timestamp, trainer.max_row_count, min_sample_for_training)
    connector.write_table(filtered_feature_table, feature_table_name_remote, write_mode="overwrite", if_exists="replace")
    logger.info("Training and fetching the results")

    try:
        train_results_json = connector._call_procedure(train_procedure,
                                                    feature_table_name_remote,
                                                    merged_config,
                                                    session=session,
                                                    connector=connector,
                                                    trainer=trainer)
    except Exception as e:
        logger.error(f"Error while training the model: {e}")
        raise e

    return train_results_json, arraytype_features, timestamp_columns

if __name__ == "__main__":
    import sys 
    import argparse 
    from MLTrainer import ClassificationTrainer, RegressionTrainer

    try:
        from RedshiftConnector import RedshiftConnector
    except Exception as e:
            logger.warning(f"Could not import RedshiftConnector")
    
    parser = argparse.ArgumentParser()
    
    parser.add_argument("--material_names", type=json.loads)
    parser.add_argument("--merged_config", type=json.loads)
    parser.add_argument("--prediction_task", type=str)
    parser.add_argument("--wh_creds", type=json.loads)
    args = parser.parse_args()

    prep_config = utils.PreprocessorConfig(**args.merged_config["preprocessing"])
    if args.prediction_task == 'classification':    
        trainer = ClassificationTrainer(**args.merged_config["data"], prep=prep_config)
    elif args.prediction_task == 'regression':
        trainer = RegressionTrainer(**args.merged_config["data"], prep=prep_config)

    ### Creating the Redshift connector and session bcoz this case of code will only be triggerred for Redshift
    train_procedure = train_and_store_model_results_rs
    connector = RedshiftConnector('./')
    session = connector.build_session(args.wh_creds)

    preprocess_and_train(train_procedure, args.material_names, args.merged_config, session=session, connector=connector, trainer=trainer)