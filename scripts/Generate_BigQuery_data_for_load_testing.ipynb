{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('./')\n",
    "sys.path.append('../')\n",
    "\n",
    "import yaml\n",
    "import pathlib\n",
    "import json\n",
    "from src.predictions.profiles_mlcorelib.utils.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.predictions.profiles_mlcorelib.connectors.BigQueryConnector import BigQueryConnector\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not import BigQueryConnector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "homedir = os.path.expanduser(\"~\")\n",
    "\n",
    "with open(os.path.join(homedir, \".pb/siteconfig.yaml\"), \"r\") as f:\n",
    "    creds = yaml.safe_load(f)[\"connections\"][\"test-bq\"][\"outputs\"][\"dev\"]\n",
    "\n",
    "if creds[\"type\"] == \"snowflake\":\n",
    "    print(\n",
    "        f\"Using {creds['schema']} schema in snowflake account: {creds['account']}\"\n",
    "    )\n",
    "elif creds[\"type\"] == \"redshift\":\n",
    "    print(f\"Using {creds['schema']} schema in Redshift account: {creds['host']}\")\n",
    "elif creds[\"type\"] == \"bigquery\":\n",
    "    print(\n",
    "        f\"Using {creds['schema']} schema in BigQuery project: {creds['project_id']}\"\n",
    "    )\n",
    "else:\n",
    "    raise Exception(f\"Unknown database type: {creds['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Use schema from where we are fetching data in projects\n",
    "\n",
    "creds[\"schema\"] = \"<NAME OF SCHEMA HAVING INPUT TABLES>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "connector = BigQueryConnector(creds, \"./\")\n",
    "session = connector.build_session(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "type(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pages = \"pages\"\n",
    "tracks = \"tracks\"\n",
    "identifies = \"identifies\"\n",
    "order_completed = \"order_completed\"\n",
    "\n",
    "strings_to_remove = ['ANONYMOUS_ID', 'USER_ID', 'TIMESTAMP', 'EMAIL']\n",
    "uppercase_list = lambda features: [feature.upper() for feature in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Pages Table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch all pages table column names as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fetch_column_names_query = f\"select column_name from `{creds['project_id']}.{creds['schema']}.INFORMATION_SCHEMA.COLUMNS` where table_name = '{pages}'\"\n",
    "column_names_list = [row.column_name for row in connector.run_query(fetch_column_names_query)]\n",
    "column_names_list = uppercase_list(column_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for string in strings_to_remove:\n",
    "    if string in column_names_list:\n",
    "        column_names_list.remove(string)\n",
    "\n",
    "column_name_string = ', '.join(column_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pages table copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_create_temp = f\"\"\"\n",
    "    CREATE TABLE `{creds['schema']}.{pages}_1` \n",
    "    AS (\n",
    "        SELECT \n",
    "            ANONYMOUS_ID, \n",
    "            USER_ID, \n",
    "            TIMESTAMP as TIMESTAMP, \n",
    "            {column_name_string}\n",
    "        FROM `{creds['schema']}.{pages}`\n",
    "    );\n",
    "\"\"\"\n",
    "connector.run_query(query_create_temp, response=False)\n",
    "\n",
    "print(\"Created temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "iter = 1\n",
    "data = {\"100k\": 100000,} # \"500k\": 500000, \"1mn\": 1000000}\n",
    "query_count_row = f\"select count(distinct ANONYMOUS_ID) as ROW_COUNT from `{creds['schema']}.{pages}_1`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for postfix, limit in data.items():\n",
    "    # reaching limits one by one.\n",
    "    while int(connector.run_query(query_count_row)[0].ROW_COUNT) < limit:\n",
    "        query_extend_tmp = f\"\"\"\n",
    "            INSERT INTO `{creds['schema']}.{pages}_1` (\n",
    "                (select \n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(ANONYMOUS_ID AS STRING), {str(iter)})))) as ANONYMOUS_ID,\n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(USER_ID AS STRING), {str(iter)})))) as USER_ID, \n",
    "                    TIMESTAMP_ADD(T.TIMESTAMP, INTERVAL CAST(RAND() * 28 AS INT64) DAY) AS TIMESTAMP,\n",
    "                    {column_name_string}\n",
    "                from `{creds['schema']}.{pages}_1` T)\n",
    "            );\n",
    "        \"\"\"\n",
    "        connector.run_query(query_extend_tmp, response=False)\n",
    "        iter += 1\n",
    "    \n",
    "    #saving the limit results.\n",
    "    query_save_limit = f\"\"\"\n",
    "        CREATE TABLE `{creds['schema']}.{pages}_{postfix}`\n",
    "        AS (\n",
    "            SELECT \n",
    "                ANONYMOUS_ID, \n",
    "                USER_ID, \n",
    "                TIMESTAMP as TIMESTAMP, \n",
    "                {column_name_string}\n",
    "            FROM `{creds['schema']}.{pages}_1`\n",
    "        );\n",
    "    \"\"\"\n",
    "    connector.run_query(query_save_limit, response=False)\n",
    "    print(f\"Saved {limit} rows successfully in table {pages}_{postfix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping extra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_drop_temp = f\"DROP TABLE `{creds['schema']}.{pages}_1`\"\n",
    "connector.run_query(query_drop_temp, response=False)\n",
    "\n",
    "print(\"Dropped temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Tracks Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fetch_column_names_query = f\"select column_name from `{creds['project_id']}.{creds['schema']}.INFORMATION_SCHEMA.COLUMNS` where table_name = '{tracks}'\"\n",
    "column_names_list = [row.column_name for row in connector.run_query(fetch_column_names_query)]\n",
    "column_names_list = uppercase_list(column_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for string in strings_to_remove:\n",
    "    if string in column_names_list:\n",
    "        column_names_list.remove(string)\n",
    "\n",
    "column_name_string = ', '.join(column_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tracks table copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_create_temp = f\"\"\"\n",
    "    CREATE TABLE `{creds['schema']}.{tracks}_1` \n",
    "    AS (\n",
    "        SELECT \n",
    "            ANONYMOUS_ID, \n",
    "            USER_ID, \n",
    "            TIMESTAMP as TIMESTAMP, \n",
    "            {column_name_string}\n",
    "        FROM `{creds['schema']}.{tracks}`\n",
    "    );\n",
    "\"\"\"\n",
    "connector.run_query(query_create_temp, response=False)\n",
    "\n",
    "print(\"Created temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "iter = 1\n",
    "data = {\"100k\": 100000,}    # \"500k\": 500000, \"1mn\": 1000000}\n",
    "query_count_row = f\"select count(distinct ANONYMOUS_ID) as ROW_COUNT from `{creds['schema']}.{tracks}_1`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for postfix, limit in data.items():\n",
    "    # reaching limits one by one.\n",
    "    while int(connector.run_query(query_count_row)[0].ROW_COUNT) < limit:\n",
    "        query_extend_tmp = f\"\"\"\n",
    "            INSERT INTO `{creds['schema']}.{tracks}_1` (\n",
    "                (select \n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(ANONYMOUS_ID AS STRING), {str(iter)})))) as ANONYMOUS_ID,\n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(USER_ID AS STRING), {str(iter)})))) as USER_ID, \n",
    "                    TIMESTAMP_ADD(T.TIMESTAMP, INTERVAL CAST(RAND() * 28 AS INT64) DAY) AS TIMESTAMP,\n",
    "                    {column_name_string}\n",
    "                from `{creds['schema']}.{tracks}_1` T)\n",
    "            );\n",
    "        \"\"\"\n",
    "        connector.run_query(query_extend_tmp, response=False)\n",
    "        iter += 1\n",
    "    \n",
    "    #saving the limit results.\n",
    "    query_save_limit = f\"\"\"\n",
    "        CREATE TABLE `{creds['schema']}.{tracks}_{postfix}`\n",
    "        AS (\n",
    "            SELECT \n",
    "                ANONYMOUS_ID, \n",
    "                USER_ID, \n",
    "                TIMESTAMP as TIMESTAMP, \n",
    "                {column_name_string}\n",
    "            FROM `{creds['schema']}.{tracks}_1`\n",
    "        );\n",
    "    \"\"\"\n",
    "    connector.run_query(query_save_limit, response=False)\n",
    "    print(f\"Saved {limit} rows successfully in table {tracks}_{postfix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping extra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_drop_temp = f\"DROP TABLE `{creds['schema']}.{tracks}_1`\"\n",
    "connector.run_query(query_drop_temp, response=False)\n",
    "\n",
    "print(\"Dropped temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For identifies Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fetch_column_names_query = f\"select column_name from `{creds['project_id']}.{creds['schema']}.INFORMATION_SCHEMA.COLUMNS` where table_name = '{identifies}'\"\n",
    "column_names_list = [row.column_name for row in connector.run_query(fetch_column_names_query)]\n",
    "column_names_list = uppercase_list(column_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for string in strings_to_remove:\n",
    "    if string in column_names_list:\n",
    "        column_names_list.remove(string)\n",
    "\n",
    "column_name_string = ', '.join(column_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating identifies table copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_create_temp = f\"\"\"\n",
    "    CREATE TABLE `{creds['schema']}.{identifies}_1` \n",
    "    AS (\n",
    "        SELECT \n",
    "            ANONYMOUS_ID, \n",
    "            USER_ID, \n",
    "            EMAIL,\n",
    "            TIMESTAMP as TIMESTAMP, \n",
    "            {column_name_string}\n",
    "        FROM `{creds['schema']}.{identifies}`\n",
    "    );\n",
    "\"\"\"\n",
    "connector.run_query(query_create_temp, response=False)\n",
    "\n",
    "print(\"Created temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "iter = 1\n",
    "data = {\"100k\": 100000,}    # \"500k\": 500000, \"1mn\": 1000000}\n",
    "query_count_row = f\"select count(distinct ANONYMOUS_ID) as ROW_COUNT from `{creds['schema']}.{identifies}_1`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for postfix, limit in data.items():\n",
    "    # reaching limits one by one.\n",
    "    while int(connector.run_query(query_count_row)[0].ROW_COUNT) < limit:\n",
    "        query_extend_tmp = f\"\"\"\n",
    "            INSERT INTO `{creds['schema']}.{identifies}_1` (\n",
    "                (select \n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(ANONYMOUS_ID AS STRING), {str(iter)})))) as ANONYMOUS_ID,\n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(USER_ID AS STRING), {str(iter)})))) as USER_ID, \n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(EMAIL AS STRING), {str(iter)})))) as EMAIL, \n",
    "                    TIMESTAMP_ADD(T.TIMESTAMP, INTERVAL CAST(RAND() * 28 AS INT64) DAY) AS TIMESTAMP,\n",
    "                    {column_name_string}\n",
    "                from `{creds['schema']}.{identifies}_1` T)\n",
    "            );\n",
    "        \"\"\"\n",
    "        connector.run_query(query_extend_tmp, response=False)\n",
    "        iter += 1\n",
    "    \n",
    "    #saving the limit results.\n",
    "    query_save_limit = f\"\"\"\n",
    "        CREATE TABLE `{creds['schema']}.{identifies}_{postfix}`\n",
    "        AS (\n",
    "            SELECT \n",
    "                ANONYMOUS_ID, \n",
    "                USER_ID, \n",
    "                EMAIL,\n",
    "                TIMESTAMP as TIMESTAMP, \n",
    "                {column_name_string}\n",
    "            FROM `{creds['schema']}.{identifies}_1`\n",
    "        );\n",
    "    \"\"\"\n",
    "    connector.run_query(query_save_limit, response=False)\n",
    "    print(f\"Saved {limit} rows successfully in table {identifies}_{postfix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping extra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_drop_temp = f\"DROP TABLE `{creds['schema']}.{identifies}_1`\"\n",
    "connector.run_query(query_drop_temp, response=False)\n",
    "\n",
    "print(\"Dropped temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For order_completed Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fetch_column_names_query = f\"select column_name from `{creds['project_id']}.{creds['schema']}.INFORMATION_SCHEMA.COLUMNS` where table_name = '{order_completed}'\"\n",
    "column_names_list = [row.column_name for row in connector.run_query(fetch_column_names_query)]\n",
    "column_names_list = uppercase_list(column_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for string in strings_to_remove:\n",
    "    if string in column_names_list:\n",
    "        column_names_list.remove(string)\n",
    "\n",
    "column_name_string = ', '.join(column_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating order_completed table copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "query_create_temp = f\"\"\"\n",
    "    CREATE TABLE `{creds['schema']}.{order_completed}_1` \n",
    "    AS (\n",
    "        SELECT \n",
    "            ANONYMOUS_ID, \n",
    "            USER_ID,\n",
    "            TIMESTAMP as TIMESTAMP, \n",
    "            {column_name_string}\n",
    "        FROM `{creds['schema']}.{order_completed}`\n",
    "    );\n",
    "\"\"\"\n",
    "connector.run_query(query_create_temp, response=False)\n",
    "\n",
    "print(\"Created temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1\n",
    "data = {\"100k\": 100000, \"500k\": 500000, \"1mn\": 1000000}\n",
    "query_count_row = f\"select count(distinct ANONYMOUS_ID) as ROW_COUNT from `{creds['schema']}.{order_completed}_1`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for postfix, limit in data.items():\n",
    "    # reaching limits one by one.\n",
    "    while int(connector.run_query(query_count_row)[0].ROW_COUNT) < limit:\n",
    "        query_extend_tmp = f\"\"\"\n",
    "            INSERT INTO `{creds['schema']}.{order_completed}_1` (\n",
    "                (select \n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(ANONYMOUS_ID AS STRING), {str(iter)})))) as ANONYMOUS_ID,\n",
    "                    LOWER(TO_HEX(SHA1(CONCAT(CAST(USER_ID AS STRING), {str(iter)})))) as USER_ID,\n",
    "                    TIMESTAMP_ADD(T.TIMESTAMP, INTERVAL CAST(RAND() * 28 AS INT64) DAY) AS TIMESTAMP,\n",
    "                    {column_name_string}\n",
    "                from `{creds['schema']}.{order_completed}_1` T)\n",
    "            );\n",
    "        \"\"\"\n",
    "        connector.run_query(query_extend_tmp, response=False)\n",
    "        iter += 1\n",
    "    \n",
    "    #saving the limit results.\n",
    "    query_save_limit = f\"\"\"\n",
    "        CREATE TABLE `{creds['schema']}.{order_completed}_{postfix}`\n",
    "        AS (\n",
    "            SELECT \n",
    "                ANONYMOUS_ID, \n",
    "                USER_ID,\n",
    "                TIMESTAMP as TIMESTAMP, \n",
    "                {column_name_string}\n",
    "            FROM `{creds['schema']}.{order_completed}_1`\n",
    "        );\n",
    "    \"\"\"\n",
    "    connector.run_query(query_save_limit, response=False)\n",
    "    print(f\"Saved {limit} rows successfully in table {order_completed}_{postfix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping extra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_drop_temp = f\"DROP TABLE `{creds['schema']}.{order_completed}_1`\"\n",
    "connector.run_query(query_drop_temp, response=False)\n",
    "\n",
    "print(\"Dropped temp tables successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
