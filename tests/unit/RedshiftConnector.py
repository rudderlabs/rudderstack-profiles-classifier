# Generated by CodiumAI
import unittest
import pandas as pd
import numpy as np
from datetime import datetime
from collections import namedtuple
from unittest.mock import Mock, patch, call
from redshift_connector.cursor import Cursor
from pandas.core.api import DataFrame as DataFrame
from src.predictions.rudderstack_predictions.trainers.TrainerFactory import (
    TrainerFactory,
)
from src.predictions.rudderstack_predictions.wht.pythonWHT import PythonWHT

import src.predictions.rudderstack_predictions.utils.utils as utils
from src.predictions.rudderstack_predictions.utils.constants import TrainTablesInfo
from src.predictions.rudderstack_predictions.connectors.RedshiftConnector import (
    RedshiftConnector,
)
from tests.unit.MLTrainer import build_trainer_config


class RedshiftConnectorV2(RedshiftConnector):
    def __init__(self, folder_path):
        super().__init__({}, folder_path)

    def build_session(self, creds):
        pass


class TestGetMaterialRegistryTable(unittest.TestCase):
    # Returns a filtered material registry table containing only the successfully materialized data.
    def test_returns_filtered_material_registry_table(self):
        class MockRedshiftConnector(RedshiftConnectorV2):
            def get_table_as_dataframe(
                self, cursor: Cursor, table_name: str, **kwargs
            ) -> DataFrame:
                material_registry_table = pd.DataFrame.from_dict(
                    {
                        "seq_no": [1, 2, 3, 4, 5],
                        "metadata": [
                            '{"complete": {"status": 1}}',
                            '{"complete": {"status": 2}}',
                            None,
                            "null",
                            "{}",
                        ],
                    }
                )
                return material_registry_table

        redshift_connector = MockRedshiftConnector(folder_path="data")
        material_registry_table = redshift_connector.get_material_registry_table(
            material_registry_table_name=None
        )
        expected_registry_table = pd.DataFrame.from_dict(
            {"seq_no": [2], "metadata": ['{"complete": {"status": 2}}'], "status": [2]}
        )
        self.assertEqual(
            material_registry_table.values.all(), expected_registry_table.values.all()
        )

    def test_returns_filtered_material_registry_table_empty_resp(self):
        class MockRedshiftConnector(RedshiftConnectorV2):
            def get_table_as_dataframe(
                self, cursor: Cursor, table_name: str, **kwargs
            ) -> DataFrame:
                material_registry_table = pd.DataFrame.from_dict(
                    {
                        "seq_no": [1, 2, 3, 4, 5],
                        "metadata": [
                            '{"complete": {"status": 1}}',
                            '{"complete": {"status": 1}}',
                            None,
                            "null",
                            "{}",
                        ],
                    }
                )
                return material_registry_table

        redshift_connector = MockRedshiftConnector(folder_path="data")
        material_registry_table = redshift_connector.get_material_registry_table(
            material_registry_table_name=None
        )
        expected_registry_table = pd.DataFrame.from_dict({})
        self.assertEqual(
            material_registry_table.values.all(), expected_registry_table.values.all()
        )

    def test_runs_on_empty_material_registry_table(self):
        class MockRedshiftConnector(RedshiftConnectorV2):
            def get_table_as_dataframe(
                self, cursor: Cursor, table_name: str, **kwargs
            ) -> DataFrame:
                material_registry_table = pd.DataFrame.from_dict(
                    {"seq_no": [], "metadata": []}
                )
                return material_registry_table

        redshift_connector = MockRedshiftConnector(folder_path="data")

        # Call the get_material_registry_table method
        material_registry_table = redshift_connector.get_material_registry_table(
            material_registry_table_name=None
        )
        expected_registry_table = pd.DataFrame.from_dict({})
        self.assertEqual(
            material_registry_table.values.all(), expected_registry_table.values.all()
        )


class TestGetMaterialNames(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        self.start_date = "2022-01-01"
        self.end_date = "2022-01-31"
        self.features_profiles_model = "model_name"
        self.model_hash = "model_hash"
        self.prediction_horizon_days = 7
        self.input_models = ["model1.yaml", "model2.yaml"]
        self.inputs = ["""select * from material_user_var_736465_0"""]
        self.whtService = PythonWHT()
        self.whtService.init(self.connector, "siteconfig.yaml", "project_folder")

    def test_fetch_filtered_table(self):
        # Set up the expected input and output
        convert_to_timestamp = lambda date_string: datetime.strptime(
            date_string, "%Y-%m-%d %H:%M:%S"
        )
        input_table = pd.DataFrame(
            {
                "model_name": [
                    self.features_profiles_model,
                    self.features_profiles_model,
                    self.features_profiles_model,
                    f"NOT_{self.features_profiles_model}",
                ],
                "model_hash": [
                    self.model_hash,
                    self.model_hash,
                    self.model_hash,
                    f"NOT_{self.model_hash}",
                ],
                "end_ts": [
                    convert_to_timestamp("2022-01-01 07:29:25"),
                    convert_to_timestamp("2022-01-01 00:00:00"),
                    convert_to_timestamp("2022-01-31 00:00:00"),
                    convert_to_timestamp("2022-01-31 08:29:25"),
                ],
                "seq_no": [13, 14, 15, 16],
            }
        )
        expected_result = pd.DataFrame(
            {
                "FEATURE_SEQ_NO": [13, 14, 15],
                "FEATURE_END_TS": [
                    convert_to_timestamp("2022-01-01 07:29:25"),
                    convert_to_timestamp("2022-01-01 00:00:00"),
                    convert_to_timestamp("2022-01-31 00:00:00"),
                ],
            }
        )

        # Invoke the method under test
        result = self.connector.fetch_filtered_table(
            input_table,
            self.features_profiles_model,
            self.model_hash,
            self.start_date,
            self.end_date,
            columns={"seq_no": "FEATURE_SEQ_NO", "end_ts": "FEATURE_END_TS"},
        )

        # Assert the result
        self.assertEqual(result.to_dict(), expected_result.to_dict())

    def test_get_valid_feature_label_dates_with_both_materials(self):
        # Set up the expected input and output
        input_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="feature_table_dt",
                label_table_name="label_table_name",
                label_table_date="label_table_dt",
            ),
        ]
        material_info = input_materials[0]

        # Mock the internal method is_valid_table
        self.connector.is_valid_table = Mock(return_value=True)

        # Invoke the method under test
        with self.assertRaises(Exception) as context:
            _ = self.whtService._get_valid_feature_label_dates(
                input_materials,
                self.start_date,
                self.prediction_horizon_days,
            )
        # Check the exception message
        self.assertIn(
            f"We don't need to fetch feature_date and label_date to materialise new datasets because Tables {material_info.feature_table_name} and {material_info.label_table_name} already exist. Please check generated materials for discrepancies.",
            str(context.exception),
        )

    def test_get_valid_feature_label_dates_with_feature_materials(self):
        # Set up the expected input and output
        input_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2022-01-10",
                label_table_name=None,
                label_table_date=None,
            ),
        ]
        expected_date = (None, "2022-01-17")

        # Mock the internal method is_valid_table
        self.connector.is_valid_table = Mock(return_value=True)

        # Invoke the method under test
        dates = self.whtService._get_valid_feature_label_dates(
            input_materials,
            self.start_date,
            self.prediction_horizon_days,
        )
        # Assert the result
        self.assertEqual(dates, expected_date)

    def test_get_valid_feature_label_dates_with_label_materials(self):
        # Set up the expected input and output
        input_materials = [
            TrainTablesInfo(
                feature_table_name=None,
                feature_table_date=None,
                label_table_name="label_table_name",
                label_table_date="2022-01-20",
            ),
        ]
        expected_date = ("2022-01-13", None)

        # Mock the internal method is_valid_table
        self.connector.is_valid_table = Mock(return_value=True)

        # Invoke the method under test
        dates = self.whtService._get_valid_feature_label_dates(
            input_materials,
            self.start_date,
            self.prediction_horizon_days,
        )
        # Assert the result
        self.assertEqual(dates, expected_date)

    def test_get_valid_feature_label_dates_with_no_materials(self):
        # Set up the expected input and output
        input_materials = [
            TrainTablesInfo(
                feature_table_name=None,
                feature_table_date=None,
                label_table_name=None,
                label_table_date=None,
            ),
        ]
        expected_date = ("2022-01-08", "2022-01-15")

        # Mock the internal method is_valid_table
        self.connector.is_valid_table = Mock(return_value=True)

        # Invoke the method under test
        dates = self.whtService._get_valid_feature_label_dates(
            input_materials,
            self.start_date,
            self.prediction_horizon_days,
        )
        # Assert the result
        self.assertEqual(dates, expected_date)

    def test_generate_training_materials_with_only_feature_material(self):
        # Set up the expected input and output
        input_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="feature_table_dt",
                label_table_name=None,
                label_table_date=None,
            ),
        ]
        expected_date = (None, "2000-01-01")

        # Mock the internal method get_valid_feature_label_dates
        self.whtService._get_valid_feature_label_dates = Mock(
            return_value=expected_date
        )
        utils.subprocess_run = Mock()

        # Invoke the method under test
        self.whtService._generate_training_materials(
            input_materials,
            self.start_date,
            self.prediction_horizon_days,
            self.input_models,
        )
        utils.subprocess_run.assert_called_once_with(
            [
                "pb",
                "run",
                "-p",
                "project_folder",
                "-m",
                "model1.yaml,model2.yaml",
                "--migrate_on_load=True",
                "--end_time",
                "946684800",
                "-c",
                "siteconfig.yaml",
            ]
        )

    # Retrieves material names and training dates when materialized data is available within the specified date range
    def test_retrieves_material_names_within_date_range(self):
        # Set up the expected input and output
        input_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="feature_table_dt",
                label_table_name="label_table_name",
                label_table_date="label_table_dt",
            ),
        ]
        expected_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="feature_table_dt",
                label_table_name="label_table_name",
                label_table_date="label_table_dt",
            )
        ]

        # Mock the internal method get_material_names_
        self.whtService._get_material_names = Mock(return_value=input_materials)
        # Invoke the method under test
        materials = self.whtService.get_material_names(
            self.start_date,
            self.end_date,
            self.features_profiles_model,
            self.model_hash,
            self.prediction_horizon_days,
            self.input_models,
            self.inputs,
        )
        # Assert the result
        self.assertEqual(materials, expected_materials)

    # Materializes feature and label data if no materialized data is found within the specified date range and retrieves material names and training dates
    def test_materializes_data_if_not_found_within_date_range(self):
        # Set up the expected input and output
        input_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="feature_table_dt",
                label_table_name="label_table_name",
                label_table_date="label_table_dt",
            ),
        ]
        expected_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="feature_table_dt",
                label_table_name="label_table_name",
                label_table_date="label_table_dt",
            )
        ]
        # Mock the internal methods get_material_names_ and generate_training_materials
        self.whtService._get_material_names = Mock(side_effect=[[], input_materials])
        utils.subprocess_run = Mock()

        # Invoke the method under test
        materials = self.whtService.get_material_names(
            self.start_date,
            self.end_date,
            self.features_profiles_model,
            self.model_hash,
            self.prediction_horizon_days,
            self.input_models,
            self.inputs,
        )

        # Assert the result
        self.assertEqual(materials, expected_materials)
        utils.subprocess_run.assert_called_with(
            [
                "pb",
                "run",
                "-p",
                "project_folder",
                "-m",
                "model1.yaml,model2.yaml",
                "--migrate_on_load=True",
                "--end_time",
                "1642204800",
                "-c",
                "siteconfig.yaml",
            ]
        )

    def test_materializes_data_once_even_if_it_cant_find_right_materials(self):
        # Mock the internal methods get_material_names_ and generate_training_materials
        self.whtService._get_material_names = Mock(return_value=[])
        self.whtService._generate_training_materials = Mock()

        # Invoke the method under test and assert exception
        with self.assertRaises(Exception) as context:
            self.whtService.get_material_names(
                self.start_date,
                self.end_date,
                self.features_profiles_model,
                self.model_hash,
                self.prediction_horizon_days,
                self.input_models,
                self.inputs,
            )
        # Check the exception message
        self.assertIn(
            "Tried to materialise past data but no materialized data found",
            str(context.exception),
        )

        # Assert generate_training_materials called once
        self.whtService._generate_training_materials.assert_called_once_with(
            [],
            self.start_date,
            self.prediction_horizon_days,
            self.input_models,
        )


class TestSelectRelevantColumns(unittest.TestCase):
    # Returns a pandas DataFrame with only the columns specified in the training_features_columns dictionary.
    def test_relevant_columns_only(self):
        table = pd.DataFrame(
            {
                "col1": [1, 2, 3],
                "col2": [4, 5, 6],
                "col3": [7, 8, 9],
                "col4": [10, 11, 12],
            }
        )
        training_features_columns = ["COL1", "COL2", "COL3"]
        redshift_connector = RedshiftConnectorV2("data")
        relevant_columns = redshift_connector.select_relevant_columns(
            table, training_features_columns
        )
        expected_columns = ["col1", "col2", "col3"]
        self.assertEqual(list(relevant_columns.columns), expected_columns)

    def test_relevant_columns_only_handling_case_sensitivity(self):
        table = pd.DataFrame(
            {
                "COL1": [1, 2, 3],
                "col2": [4, 5, 6],
                "col3": [7, 8, 9],
                "col4": [10, 11, 12],
            }
        )
        training_features_columns = ["COL1", "COL2", "COL3"]
        redshift_connector = RedshiftConnectorV2("data")
        relevant_columns = redshift_connector.select_relevant_columns(
            table, training_features_columns
        )
        expected_columns = ["COL1", "col2", "col3"]
        self.assertEqual(list(relevant_columns.columns), expected_columns)

    # Throws an exception that the expected column is not found
    def test_relevant_columns_not_found(self):
        table = pd.DataFrame(
            {
                "col1": [1, 2, 3],
                "col2": [4, 5, 6],
                "col3": [7, 8, 9],
                "col4": [10, 11, 12],
            }
        )
        training_features_columns = ["COL1", "COL2", "COL3", "COL5"]
        redshift_connector = RedshiftConnectorV2("data")
        with self.assertRaises(Exception) as context:
            redshift_connector.select_relevant_columns(table, training_features_columns)
        self.assertIn(
            f"Expected columns {training_features_columns} not found in table ['COL1', 'COL2', 'COL3']",
            str(context.exception),
        )


class TestGenerateTypeHint(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        self.column_types = {
            "categorical": ["col2"],
            "numeric": ["col1"],
        }

    # Returns a list of type hints for given pandas DataFrame's fields
    def test_returns_type_hints(self):
        df = pd.DataFrame({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})
        type_hints = self.connector.generate_type_hint(df, self.column_types)
        self.assertEqual(type_hints, [float, str])

    # Handles empty DataFrame
    def test_handles_empty_dataframe(self):
        df = pd.DataFrame()
        type_hints = self.connector.generate_type_hint(df, self.column_types)
        self.assertEqual(type_hints, [])

    # Handles DataFrame with single row and column
    def test_handles_single_row_and_column(self):
        df = pd.DataFrame({"col1": [1]})
        type_hints = self.connector.generate_type_hint(df, self.column_types)
        self.assertEqual(type_hints, [float])


class TestValidations(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        df = pd.DataFrame.from_dict(
            {
                "COL1": ["a", "a"],
                "COL2": [1, 2],
                "COL3": [None, None],
                "COL4": ["a1", "b1"],
            }
        )
        self.table = df

    # Checks for assertion error if label column is not present in the feature table.
    def test_label_column_not_present(self):
        label_column = "label"
        with self.assertRaises(Exception) as context:
            self.connector.validate_columns_are_present(self.table, label_column)
        self.assertIn(
            f"Label column {label_column} is not present in the feature table.",
            str(context.exception),
            [],
        )

    # Checks if no:of columns in the feature table is less than 3, then it raises an exception.
    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.CLASSIFIER_MIN_LABEL_PROPORTION",
        new=1.0,
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.CLASSIFIER_MAX_LABEL_PROPORTION",
        new=0.0,
    )
    def test_expects_error_if_label_ratios_are_bad_classification(self):
        label_column = "COL2"
        with self.assertRaises(Exception) as context:
            self.connector.validate_class_proportions(
                self.table[["COL1", "COL2", "COL3"]],
                label_column,
            )
        error_msg = "Label: 1 - users :(50.00%)\n\tLabel: 2 - users :(50.00%)"
        self.assertIn(
            error_msg,
            str(context.exception),
            [],
        )

    def test_expects_error_if_label_count_is_low_regression(self):
        label_column = "COL1"
        with self.assertRaises(Exception) as context:
            self.connector.validate_label_distinct_values(self.table, label_column)
            distinct_values_count = self.table.groupBy(label_column).count()
            num_distinct_values = distinct_values_count.count()
            self.assertIn(
                f"Label column {label_column} has {num_distinct_values} distinct values",
                str(context.exception),
                [],
            )

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.CLASSIFIER_MIN_LABEL_PROPORTION",
        new=0.05,
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.CLASSIFIER_MAX_LABEL_PROPORTION",
        new=0.95,
    )
    def test_passes_for_good_data_classification(self):
        table = pd.DataFrame.from_dict(
            {
                "COL1": ["a", "b", "a"],
                "COL2": [1, 2, 3],
                "COL3": [None, None, None],
                "COL4": ["a1", "b1", "c1"],
            }
        )
        self.assertTrue(self.connector.validate_columns_are_present(table, "COL1"))
        self.assertTrue(self.connector.validate_class_proportions(table, "COL1"))

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.REGRESSOR_MIN_LABEL_DISTINCT_VALUES",
        new=3,
    )
    def test_passes_for_good_data_regression(self):
        table = pd.DataFrame.from_dict(
            {
                "COL1": [1, 2, 3, 4],
                "COL2": [1, 2, 3, 4],
                "COL3": [None, None, None, None],
                "COL4": ["a1", "b1", "c1", "d1"],
            }
        )
        self.assertTrue(self.connector.validate_columns_are_present(table, "COL1"))
        self.assertTrue(self.connector.validate_label_distinct_values(table, "COL1"))


class TestCheckForClassificationDataRequirement(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.MIN_NUM_OF_SAMPLES",
        new=90,
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.CLASSIFIER_MIN_LABEL_PROPORTION",
        new=0.01,
    )
    def test_enough_negative_and_total_samples(self):
        """Test when there are enough negative samples"""
        materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-20 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-27 00:00:00",
            ),
        ]
        label_column = "label"

        self.connector.run_query = Mock(side_effect=[([15],), ([100],)])
        result = self.connector.check_for_classification_data_requirement(
            materials, label_column, 1, "user"
        )

        self.assertTrue(result)
        self.connector.run_query.assert_any_call(
            """SELECT COUNT(*) FROM (SELECT user
                FROM feature_table_name) a
                INNER JOIN (SELECT * FROM label_table_name) b ON a.user = b.user
                WHERE b.label != 1""",
            response=True,
        )

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.MIN_NUM_OF_SAMPLES",
        new=90,
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.CLASSIFIER_MIN_LABEL_PROPORTION",
        new=0.01,
    )
    def test_insufficient_negative_and_total_samples(self):
        """Test when there are not enough negative samples"""
        materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-20 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-27 00:00:00",
            ),
        ]
        label_column = "label"

        self.connector.run_query = Mock(side_effect=[([9],), ([80],)])
        result = self.connector.check_for_classification_data_requirement(
            materials, label_column, 1, "user"
        )

        self.assertFalse(result)
        self.connector.run_query.assert_any_call(
            """SELECT COUNT(*) as count
                FROM label_table_name""",
            response=True,
        )

    def test_invalid_query_result(self):
        """Test with invalid query result format"""
        materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-20 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-27 00:00:00",
            ),
        ]
        label_column = "label"

        self.connector.run_query = Mock(return_value={"invalid_key": "invalid_value"})

        with self.assertRaises(KeyError):
            self.connector.check_for_classification_data_requirement(
                materials, label_column, 1, "user"
            )

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.MIN_NUM_OF_SAMPLES",
        new=100,
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.CLASSIFIER_MIN_LABEL_PROPORTION",
        new=0.01,
    )
    def test_empty_materials(self):
        """Test with empty materials list"""
        materials = []
        label_column = "label"

        self.connector.run_query = Mock()
        self.connector.run_query.assert_not_called()
        result = self.connector.check_for_classification_data_requirement(
            materials, label_column, 1, "user"
        )
        self.assertFalse(result)  # No materials, so considered sufficient


class TestCheckForRegressionDataRequirement(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.MIN_NUM_OF_SAMPLES",
        new=90,
    )
    def test_enough_total_samples(self):
        """Test when there are enough negative samples"""
        materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-20 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-27 00:00:00",
            ),
        ]

        self.connector.run_query = Mock(return_value=([100],))
        result = self.connector.check_for_regression_data_requirement(materials)

        self.assertTrue(result)
        self.connector.run_query.assert_called_once_with(
            """SELECT COUNT(*) as count
                FROM feature_table_name""",
            response=True,
        )

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.MIN_NUM_OF_SAMPLES",
        new=90,
    )
    def test_insufficient_total_samples(self):
        """Test when there are not enough negative samples"""
        materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-20 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-27 00:00:00",
            ),
        ]

        self.connector.run_query = Mock(return_value=([49],))
        result = self.connector.check_for_regression_data_requirement(materials)

        self.assertFalse(result)
        self.connector.run_query.assert_called_once_with(
            """SELECT COUNT(*) as count
                FROM feature_table_name""",
            response=True,
        )

    def test_invalid_query_result(self):
        """Test with invalid query result format"""
        materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-20 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-27 00:00:00",
            ),
        ]

        self.connector.run_query = Mock(return_value={"invalid_key": "invalid_value"})

        with self.assertRaises(KeyError):
            self.connector.check_for_regression_data_requirement(materials)

    @patch(
        "src.predictions.rudderstack_predictions.utils.constants.MIN_NUM_OF_SAMPLES",
        new=100,
    )
    def test_empty_materials(self):
        """Test with empty materials list"""
        materials = []

        self.connector.run_query = Mock()
        self.connector.run_query.assert_not_called()
        result = self.connector.check_for_regression_data_requirement(materials)
        self.assertFalse(result)  # No materials, so considered sufficient


class TestCheckAndGenerateMoreMaterials(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        self.materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-20 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-27 00:00:00",
            ),
        ]
        self.whtService = PythonWHT()
        self.whtService.init(self.connector, "siteconfig.yaml", "project_folder")
        trainer_input = build_trainer_config()
        self.trainer = TrainerFactory.create(trainer_input)
        self.trainer.materialisation_strategy = "auto"
        self.trainer.materialisation_max_no_dates = 2
        self.prediction_horizon_days = 7
        self.trainer.prediction_horizon_days = self.prediction_horizon_days
        self.feature_data_min_date_diff = 0
        self.trainer.feature_data_min_date_diff = self.feature_data_min_date_diff
        self.trainer.materialisation_dates = []
        self.input_models = "model_name"

    @patch("src.predictions.rudderstack_predictions.utils.utils.date_add")
    @patch("src.predictions.rudderstack_predictions.utils.utils.dates_proximity_check")
    @patch("src.predictions.rudderstack_predictions.utils.utils.get_abs_date_diff")
    @patch(
        "src.predictions.rudderstack_predictions.utils.utils.get_feature_package_path"
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.utils.datetime_to_date_string"
    )
    @patch("src.predictions.rudderstack_predictions.wht.rudderPB.RudderPB.run")
    def test_generate_new_materials_auto_strategy(
        self,
        mock_rudderpb_run,
        mock_datetime_to_date_string,
        mock_get_feature_package_path,
        mock_get_abs_date_diff,
        mock_dates_proximity_check,
        mock_date_add,
    ):
        # Mock data
        mock_rudderpb_run.return_value = True
        mock_datetime_to_date_string.side_effect = ["2024-02-20", "2024-02-20"]
        mock_get_feature_package_path.return_value = "feature_package_path"
        new_materials = [
            TrainTablesInfo(
                feature_table_name="feature_table_name",
                feature_table_date="2024-02-06 00:00:00",
                label_table_name="label_table_name",
                label_table_date="2024-02-13 00:00:00",
            ),
        ]

        mock_get_material_func = Mock(
            side_effect=[[], new_materials, [], new_materials]
        )

        mock_date_add.side_effect = [
            "2024-02-06",
            "2024-02-13",
            "2024-02-06",
            "2024-02-13",
        ]

        # Call the function
        self.trainer.check_min_data_requirement = Mock(side_effect=[False, True, True])
        result = self.trainer.check_and_generate_more_materials(
            mock_get_material_func,
            materials=self.materials.copy(),
            input_models=self.input_models,
            whtService=self.whtService,
            connector=self.connector,
        )

        # Assertions
        self.assertEqual(len(result), 2)  # Two materials generated

        # Verify calls to mock functions
        mock_get_feature_package_path.assert_called_once_with(self.input_models)
        mock_rudderpb_run.assert_called()  # Called twice

        mock_dates_proximity_check.assert_called_once_with(
            "2024-02-06", ["2024-02-20"], self.feature_data_min_date_diff
        )
        mock_date_add.assert_has_calls(
            [
                call("2024-02-20", -1 * self.feature_data_min_date_diff),
                call("2024-02-06", self.prediction_horizon_days),
            ]
        )
        mock_datetime_to_date_string.assert_called_once()
        mock_get_abs_date_diff.assert_called_once()

        # Test early termination due to materialisation failure
        mock_rudderpb_run.side_effect = ValueError

        self.trainer.check_min_data_requirement = Mock(return_value=False)
        result = self.trainer.check_and_generate_more_materials(
            mock_get_material_func,
            materials=self.materials.copy(),
            input_models=self.input_models,
            whtService=self.whtService,
            connector=self.connector,
        )

        self.assertEqual(len(result), 1)  # Only one material generated

    @patch(
        "src.predictions.rudderstack_predictions.utils.utils.get_feature_package_path"
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.utils.datetime_to_date_string"
    )
    @patch(
        "src.predictions.rudderstack_predictions.utils.utils.generate_new_training_dates"
    )
    @patch("src.predictions.rudderstack_predictions.wht.rudderPB.RudderPB.run")
    def test_generate_new_materials_manual_strategy(
        self,
        mock_rudderpb_run,
        mock_generate_new_training_dates,
        mock_datetime_to_date_string,
        mock_get_feature_package_path,
    ):
        materials_1 = [
            TrainTablesInfo(
                feature_table_name="feature_table_name_1",
                feature_table_date="2024-01-01 00:00:00",
                label_table_name="label_table_name_1",
                label_table_date="2024-01-08 00:00:00",
            ),
        ]

        materials_2_partial = [
            TrainTablesInfo(
                feature_table_name="feature_table_name_2",
                feature_table_date="2024-02-01 00:00:00",
                label_table_name=None,
                label_table_date=None,
            ),
        ]

        materials_2 = [
            TrainTablesInfo(
                feature_table_name="feature_table_name_2",
                feature_table_date="2024-02-01 00:00:00",
                label_table_name="label_table_name_2",
                label_table_date="2024-02-08 00:00:00",
            ),
        ]

        mock_rudderpb_run.side_effect = [True, True, True]
        mock_get_material_func = Mock(
            side_effect=[[], materials_1, materials_2_partial, materials_2]
        )
        mock_get_feature_package_path.return_value = "feature_package_path"

        self.trainer.materialisation_dates = [
            "2024-01-01,2024-01-08",
            "2024-02-01,2024-02-08",
        ]
        self.trainer.check_min_data_requirement = Mock(side_effect=[False, False, True])
        self.trainer.materialisation_strategy = "manual"
        result = self.trainer.check_and_generate_more_materials(
            mock_get_material_func,
            materials=self.materials.copy(),
            input_models=self.input_models,
            whtService=self.whtService,
            connector=self.connector,
        )

        # Assertions
        # We should be making only 3 calls to pb run, since one of the material dates is already present
        # in the registry
        self.assertEqual(mock_rudderpb_run.call_count, 3)  # Three materials generated

        self.assertEqual(len(result), 3)  # Three materials generated

        # Verify calls to mock functions
        mock_get_feature_package_path.assert_called_once_with(self.input_models)
        mock_datetime_to_date_string.assert_not_called()  # Not called
        mock_generate_new_training_dates.assert_not_called()  # Not called
        mock_rudderpb_run.assert_called()

        # Test case where data requirement is not met after materialisation
        mock_rudderpb_run.side_effect = ValueError
        self.trainer.check_min_data_requirement = Mock(return_value=False)

        result = self.trainer.check_and_generate_more_materials(
            mock_get_material_func,
            materials=self.materials.copy(),
            input_models=self.input_models,
            whtService=self.whtService,
            connector=self.connector,
        )

        self.assertEqual(len(result), 1)  # Only one material generated


class TestValidateHistoricalMaterialsHash(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        self.material_table = "material_table"
        self.start_date = "2022-01-01"
        self.end_date = "2022-01-31"
        self.features_profiles_model = "model_name"
        self.model_hash = "model_hash"
        self.prediction_horizon_days = 7
        self.site_config_path = "siteconfig.yaml"
        self.project_folder = "project_folder"
        self.input_models = ["model1.yaml", "model2.yaml"]
        self.inputs = ["""select * from material_user_var_736465_0"""]
        self.whtService = PythonWHT()
        self.whtService.init(self.connector, "", "")
        self.connector.get_tables_by_prefix = Mock(return_value=["material_table_1"])

    # The method is called with valid arguments and all tables exist in the warehouse registry.
    def test_valid_arguments_all_tables_exist(self):
        self.connector.check_table_entry_in_material_registry = Mock(return_value=True)
        result = self.whtService._validate_historical_materials_hash(
            "SELECT * FROM material_table_3", 1, 2
        )
        self.assertTrue(result)

    def test_valid_arguments_some_tables_dont_exist(self):
        self.connector.check_table_entry_in_material_registry = Mock(
            side_effect=[True, False]
        )
        result = self.whtService._validate_historical_materials_hash(
            "SELECT * FROM material_table_3", 1, 2
        )
        self.assertFalse(result)


class TestArrayTransformation(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        self.df_with_array_features = pd.DataFrame.from_dict(
            {
                "COL1": [1, 2, 3, 4],
                "COL2": ["value1", "value2", "value3", "value4"],
                "COL3": [
                    ["p1"],
                    ["p1", "p1", "p3", "p4", "p4", "p4", "p4"],
                    ["p1", "p2", "p3", "p3", "p1"],
                    [],
                ],
                "COL4": [
                    ["a1", "a2", "a3"],
                    ["a3", "a2", "a3"],
                    ["a4", "a2", "a3"],
                    ["a1", "a2", "a3"],
                ],
            }
        )
        self.arraytype_features = ["COL3", "COL4"]

    def test_df_with_array_features(self):
        (
            transformed_array_features,
            transformed_df,
        ) = self.connector.transform_arraytype_features(
            self.df_with_array_features,
            self.arraytype_features,
            top_k_array_categories=2,
        )

        # Check column names
        self.assertEqual(
            list(transformed_df.columns),
            [
                "COL1",
                "COL2",
                "COL3_P1",
                "COL3_P4",
                "COL3_OTHERS",
                "COL4_A2",
                "COL4_A3",
                "COL4_OTHERS",
            ],
        )
        self.assertEqual(
            list(transformed_array_features),
            ["COL3_P1", "COL3_P4", "COL3_OTHERS", "COL4_A2", "COL4_A3", "COL4_OTHERS"],
        )

    def test_max_categories(self):
        (
            transformed_array_features,
            transformed_df,
        ) = self.connector.transform_arraytype_features(
            self.df_with_array_features,
            self.arraytype_features,
            top_k_array_categories=10,
        )

        # Check column names
        self.assertEqual(
            list(transformed_df.columns),
            [
                "COL1",
                "COL2",
                "COL3_P1",
                "COL3_P2",
                "COL3_P3",
                "COL3_P4",
                "COL3_OTHERS",
                "COL4_A1",
                "COL4_A2",
                "COL4_A3",
                "COL4_A4",
                "COL4_OTHERS",
            ],
        )
        self.assertEqual(
            list(transformed_array_features),
            [
                "COL3_P1",
                "COL3_P3",
                "COL3_P4",
                "COL3_P2",
                "COL3_OTHERS",
                "COL4_A1",
                "COL4_A2",
                "COL4_A3",
                "COL4_A4",
                "COL4_OTHERS",
            ],
        )

    def test_df_with_empty_array_features(self):
        df_with_empty_array_features = pd.DataFrame.from_dict(
            {
                "COL1": [1, 2, 3, 4],
                "COL2": ["value1", "value2", "value3", "value4"],
                "COL3": [None, [], [], []],
                "COL4": [[], [], [], []],
            }
        )
        arraytype_features = ["COL3", "COL4"]

        (
            transformed_array_features,
            transformed_df,
        ) = self.connector.transform_arraytype_features(
            df_with_empty_array_features, arraytype_features, top_k_array_categories=2
        )

        # Check column names
        self.assertEqual(
            list(transformed_df.columns), ["COL1", "COL2", "COL3_OTHERS", "COL4_OTHERS"]
        )
        self.assertEqual(
            list(transformed_array_features), ["COL3_OTHERS", "COL4_OTHERS"]
        )

    def test_df_with_no_array_features(self):
        df_with_no_array_features = pd.DataFrame.from_dict(
            {
                "COL1": [1, 2, 3, 4],
                "COL2": ["value1", "value2", "value3", "value4"],
                "COL3": [3, 4, 5, 6],
                "COL4": ["a1", "a2", "a3", "a4"],
            }
        )
        arraytype_features = []

        (
            transformed_array_features,
            transformed_df,
        ) = self.connector.transform_arraytype_features(
            df_with_no_array_features, arraytype_features, top_k_array_categories=2
        )

        # Check column names
        self.assertEqual(list(transformed_df.columns), ["COL1", "COL2", "COL3", "COL4"])
        self.assertEqual(list(transformed_array_features), [])

    def test_array_df_train_and_predict_1(self):
        train_max_categroies_df = pd.DataFrame.from_dict(
            {
                "INDEX": [1, 2, 3],
                "ARRAY_COL": [
                    ["a", "a", "b"],
                    ["a", "b", "c"],
                    ["d", "e", "f"],
                ],
            }
        )

        predict_max_categroies_df_case_1 = pd.DataFrame.from_dict(
            {
                "INDEX": [1, 2, 3, 4],
                "ARRAY_COL": [
                    ["c", "c", "b"],
                    ["a", "d", "d"],
                    ["k", "j", "l"],
                    ["g", "h", "i"],
                ],
            }
        )

        predict_max_categroies_df_case_2 = pd.DataFrame.from_dict(
            {
                "INDEX": [1, 2],
                "ARRAY_COL": [
                    ["k", "k", "l"],
                    ["g", "h", "i"],
                ],
            }
        )

        arraytype_features = ["ARRAY_COL"]

        (
            transformed_array_features_train,
            transformed_df_train,
        ) = self.connector.transform_arraytype_features(
            train_max_categroies_df, arraytype_features, top_k_array_categories=2
        )

        train_arraytype_cols = {
            word: [
                item
                for item in transformed_array_features_train
                if item.startswith(word)
            ]
            for word in arraytype_features
        }

        # Test for case 1
        (
            transformed_array_features_predict,
            transformed_df_predict,
        ) = self.connector.transform_arraytype_features(
            predict_max_categroies_df_case_1,
            arraytype_features,
            top_k_array_categories=2,
            predict_arraytype_features=train_arraytype_cols,
        )

        self.assertEqual(
            list(set(transformed_df_train.columns)),
            list(set(transformed_df_predict.columns)),
        )

        # Test for case 2

        (
            transformed_array_features_predict,
            transformed_df_predict,
        ) = self.connector.transform_arraytype_features(
            predict_max_categroies_df_case_2,
            arraytype_features,
            top_k_array_categories=2,
            predict_arraytype_features=train_arraytype_cols,
        )

        self.assertEqual(
            list(set(transformed_df_train.columns)),
            list(set(transformed_df_predict.columns)),
        )

    def test_array_df_train_and_predict_2(self):
        # Test for case 1
        train_max_categroies_df_case_1 = pd.DataFrame.from_dict(
            {
                "INDEX": [1, 2, 3],
                "ARRAY_COL": [
                    [],
                    [],
                    [],
                ],
            }
        )

        predict_max_categroies_df_case_1 = pd.DataFrame.from_dict(
            {
                "INDEX": [1, 2, 3, 4],
                "ARRAY_COL": [
                    ["c", "c", "b"],
                    ["a", "d", "d"],
                    ["k", "j", "l"],
                    ["g", "h", "i"],
                ],
            }
        )

        arraytype_features = ["ARRAY_COL"]

        (
            transformed_array_features_train_case_1,
            transformed_df_train_case_1,
        ) = self.connector.transform_arraytype_features(
            train_max_categroies_df_case_1, arraytype_features, top_k_array_categories=2
        )

        train_arraytype_cols = {
            word: [
                item
                for item in transformed_array_features_train_case_1
                if item.startswith(word)
            ]
            for word in arraytype_features
        }

        (
            transformed_array_features_predict_case_1,
            transformed_df_predict_case_1,
        ) = self.connector.transform_arraytype_features(
            predict_max_categroies_df_case_1,
            arraytype_features,
            top_k_array_categories=2,
            predict_arraytype_features=train_arraytype_cols,
        )

        self.assertEqual(
            list(set(transformed_df_train_case_1.columns)),
            list(set(transformed_df_predict_case_1.columns)),
        )

        # # Test for case 2
        train_max_categroies_df_case_2 = pd.DataFrame.from_dict(
            {
                "INDEX": [1, 2, 3, 4],
                "ARRAY_COL": [
                    ["c", "c", "b"],
                    ["a", "d", "d"],
                    ["k", "j", "l"],
                    ["g", "h", "i"],
                ],
            }
        )

        predict_max_categroies_df_case_2 = pd.DataFrame.from_dict(
            {
                "INDEX": [1, 2, 3],
                "ARRAY_COL": [
                    [],
                    [],
                    [],
                ],
            }
        )

        arraytype_features = ["ARRAY_COL"]

        (
            transformed_array_features_train_case_2,
            transformed_df_train_case_2,
        ) = self.connector.transform_arraytype_features(
            train_max_categroies_df_case_2, arraytype_features, top_k_array_categories=2
        )

        train_arraytype_cols = {
            word: [
                item
                for item in transformed_array_features_train_case_2
                if item.startswith(word)
            ]
            for word in arraytype_features
        }

        (
            transformed_array_features_predict_case_2,
            transformed_df_predict_case_2,
        ) = self.connector.transform_arraytype_features(
            predict_max_categroies_df_case_2,
            arraytype_features,
            top_k_array_categories=2,
            predict_arraytype_features=train_arraytype_cols,
        )

        self.assertEqual(
            sorted(transformed_df_train_case_2.columns),
            sorted(transformed_df_predict_case_2.columns),
        )


class Testget_input_column_types(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        self.trainer_input = build_trainer_config()
        self.trainer_input["data"]["label_column"] = "COL6"
        self.trainer_input["data"]["entity_column"] = "COL1"

    def test_get_input_column_types(self):
        self.trainer_input["preprocessing"]["timestamp_columns"] = ["COL2"]
        self.trainer_input["preprocessing"]["arraytype_columns"] = ["COL3"]
        self.trainer_input["preprocessing"]["booleantype_columns"] = ["COL8"]
        self.trainer_input["preprocessing"]["numeric_pipeline"] = {
            "numeric_columns": ["COL4", "COL7"]
        }
        self.trainer_input["preprocessing"]["categorical_pipeline"] = {
            "categorical_columns": ["COL5"]
        }
        self.trainer = TrainerFactory.create(self.trainer_input)

        schema_fields = namedtuple("schema_field", ["name", "field_type"])
        named_schema_list = [
            schema_fields(name="COL4", field_type="integer"),
            schema_fields(name="COL5", field_type="integer"),
            schema_fields(name="COL7", field_type="integer"),
            schema_fields(name="COL2", field_type="date"),
            schema_fields(name="COL3", field_type="array"),
            schema_fields(name="COL8", field_type="boolean"),
        ]

        # Mock the fetch_table_metadata
        self.connector.fetch_table_metadata = Mock(return_value=named_schema_list)

        output = self.connector.get_input_column_types(
            self.trainer,
            "dummy",
            self.trainer.label_column,
            self.trainer.entity_column,
            self.trainer.prep.ignore_features,
        )

        expected = {
            "numeric": ["COL4", "COL7"],
            "categorical": ["COL5"],
            "arraytype": ["COL3"],
            "timestamp": ["COL2"],
            "booleantype": ["COL8"],
        }

        all_keys = set(expected.keys()).union(output.keys())
        expected = {key: expected.get(key, []) for key in all_keys}
        output = {key: output.get(key, []) for key in all_keys}

        for key in expected.keys():
            self.assertEqual(sorted(output[key]), sorted(expected[key]))


class TestTransformBooleantypeFeatures(unittest.TestCase):
    def setUp(self) -> None:
        self.connector = RedshiftConnectorV2("data")
        self.feature_df = pd.DataFrame(
            {
                "col1": [None, None, None, None, None],
                "col2": [True, False, None, None, True],
                "col3": [1, 2, 3, 4, 5],
                "col4": [True, False, True, True, False],
            }
        )
        self.booleantype_features = ["col1", "col2", "col4"]

    def test_transform_booleantype_features(self):
        output = self.connector.transform_booleantype_features(
            self.feature_df, self.booleantype_features
        )
        expected_output = pd.DataFrame(
            {
                "col1": [None, None, None, None, None],
                "col2": [1, 0, None, None, 1],
                "col3": [1, 2, 3, 4, 5],
                "col4": [1, 0, 1, 1, 0],
            }
        )
        output = output.replace({np.nan: None})
        expected_output = expected_output.replace({np.nan: None})

        # Check column names
        self.assertEqual(
            sorted(list(output.columns)),
            sorted(["col1", "col2", "col3", "col4"]),
        )

        # Check column values
        for col in output:
            self.assertEqual(output[col].tolist(), expected_output[col].tolist())
